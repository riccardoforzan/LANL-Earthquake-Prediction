{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/riccardorampon/LANL-Earthquake-Prediction/blob/main/EarthquakePredict.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "8oCDOob4Zo9e"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import time\n",
        "from datetime import datetime"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "OvUwx7nlZo9i"
      },
      "outputs": [],
      "source": [
        "#Extract data from CSV\n",
        "df1=pd.read_csv(\"database.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "eMzOrYP-Zo9j"
      },
      "outputs": [],
      "source": [
        "epoch = datetime(1970, 1, 1)\n",
        "\n",
        "def mapdateTotime(x):\n",
        "    try:\n",
        "        dt = datetime.strptime(x, \"%m/%d/%Y\")\n",
        "    except ValueError:\n",
        "        dt = datetime.strptime(x, \"%Y-%m-%dT%H:%M:%S.%fZ\")\n",
        "    diff = dt - epoch\n",
        "    return diff.total_seconds()\n",
        "\n",
        "df1.Date = df1.Date.apply(mapdateTotime)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5od0m5sBZo9k",
        "outputId": "e228f8a7-3d02-47c2-bf0e-03def203bd43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[-1.5768000e+08  1.9246000e+01  1.4561600e+02  1.3160000e+02]\n",
            " [-1.5750720e+08  1.8630000e+00  1.2735200e+02  8.0000000e+01]\n",
            " [-1.5742080e+08 -2.0579000e+01 -1.7397200e+02  2.0000000e+01]\n",
            " ...\n",
            " [ 1.4828832e+09  3.6917900e+01  1.4042620e+02  1.0000000e+01]\n",
            " [ 1.4829696e+09 -9.0283000e+00  1.1866390e+02  7.9000000e+01]\n",
            " [ 1.4830560e+09  3.7397300e+01  1.4141030e+02  1.1940000e+01]]\n"
          ]
        }
      ],
      "source": [
        "col1 = df1[['Date','Latitude','Longitude','Depth']]\n",
        "col2 = df1['Magnitude']\n",
        "\n",
        "#Convert to Numpy array\n",
        "InputX1 = col1.to_numpy()\n",
        "InputY1 = col2.to_numpy()\n",
        "print(InputX1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLL_8Ir5Zo9l",
        "outputId": "2ad2d228-fe70-4b36-9a41-504a9cff66f7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mininum values: [-1.57680e+08 -7.70800e+01 -1.79997e+02 -1.10000e+00]\n",
            "Maximum values: [1.483056e+09 8.600500e+01 1.799980e+02 7.000000e+02]\n"
          ]
        }
      ],
      "source": [
        "#Min-max Normalization\n",
        "X1_min = np.amin(InputX1,0)     \n",
        "X1_max = np.amax(InputX1,0)   \n",
        "print(\"Mininum values:\",X1_min)\n",
        "print(\"Maximum values:\",X1_max)\n",
        "Y1_min = np.amin(InputY1)     \n",
        "Y1_max = np.amax(InputY1) \n",
        "InputX1_norm = (InputX1-X1_min)/(X1_max-X1_min)\n",
        "InputY1_norm = InputY1  #No normalization in output\n",
        "\n",
        "#Reshape\n",
        "Xfeatures = 3 #Number of input features\n",
        "Yfeatures = 1 #Number of input features\n",
        "samples = 23000 # Number of samples\n",
        "\n",
        "InputX1_reshape = np.resize(InputX1_norm,(samples,Xfeatures))\n",
        "InputY1_reshape = np.resize(InputY1_norm,(samples,Yfeatures))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "S_k_2EFXZo9m"
      },
      "outputs": [],
      "source": [
        "#Training data\n",
        "batch_size = 2000\n",
        "InputX1train = InputX1_reshape[0:batch_size,:]\n",
        "InputY1train = InputY1_reshape[0:batch_size,:]\n",
        "#Validation data\n",
        "v_size = 2500\n",
        "InputX1v = InputX1_reshape[batch_size:batch_size+v_size,:]\n",
        "InputY1v = InputY1_reshape[batch_size:batch_size+v_size,:]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "T2u8Js-_Zo9n"
      },
      "outputs": [],
      "source": [
        "learning_rate = 0.001\n",
        "training_iterations = 1000\n",
        "display_iterations = 200"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "Z3WKvgxiZo9o"
      },
      "outputs": [],
      "source": [
        "tf.compat.v1.disable_eager_execution()\n",
        "#Input\n",
        "X = tf.compat.v1.placeholder(tf.compat.v1.float32,shape=(None,Xfeatures))\n",
        "\n",
        "#Output\n",
        "Y = tf.compat.v1.placeholder(tf.compat.v1.float32)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "c-LRu4OvZo9p"
      },
      "outputs": [],
      "source": [
        "#Neurons\n",
        "L1 = 3\n",
        "L2 = 3\n",
        "L3 = 3\n",
        "\n",
        "#Layer1 weights\n",
        "W_fc1 = tf.Variable(tf.random.uniform([Xfeatures,L1]))\n",
        "b_fc1 = tf.Variable(tf.constant(0.1,shape=[L1]))\n",
        "\n",
        "#Layer2 weights\n",
        "W_fc2 = tf.Variable(tf.random.uniform([L1,L2]))\n",
        "b_fc2 = tf.Variable(tf.constant(0.1,shape=[L2]))\n",
        "\n",
        "#Layer3 weights\n",
        "W_fc3 = tf.Variable(tf.random.uniform([L2,L3]))\n",
        "b_fc3 = tf.Variable(tf.constant(0.1,shape=[L3]))\n",
        "\n",
        "#Output layer weights\n",
        "W_fO= tf.Variable(tf.random.uniform([L3,Yfeatures]))\n",
        "b_fO = tf.Variable(tf.constant(0.1,shape=[Yfeatures]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "nD1WE1hRZo9q"
      },
      "outputs": [],
      "source": [
        "#Layer 1\n",
        "matmul_fc1=tf.matmul(X, W_fc1) + b_fc1\n",
        "h_fc1 = tf.nn.relu(matmul_fc1)   #ReLU activation\n",
        "#Layer 2\n",
        "matmul_fc2=tf.matmul(h_fc1, W_fc2) + b_fc2\n",
        "h_fc2 = tf.nn.relu(matmul_fc2)   #ReLU activation\n",
        "#Layer 3\n",
        "matmul_fc3=tf.matmul(h_fc2, W_fc3) + b_fc3\n",
        "h_fc3 = tf.nn.relu(matmul_fc3)   #ReLU activation\n",
        "#Output layer\n",
        "matmul_fc4=tf.matmul(h_fc3, W_fO) + b_fO\n",
        "output_layer = matmul_fc4  #linear activation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "HyTwdLk_Zo9r"
      },
      "outputs": [],
      "source": [
        "#Loss function\n",
        "mean_square =  tf.reduce_mean(tf.square(Y-output_layer))\n",
        "train_step = tf.compat.v1.train.AdamOptimizer(learning_rate).minimize(mean_square)\n",
        "#learning_rate).minimize(mean_square\n",
        "#Operation to save variables\n",
        "saver = tf.compat.v1.train.Saver()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MBTd2AlXZo9s",
        "outputId": "8de6b433-26f1-438f-ecdc-47990195786c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: [18.109188]\n",
            "Training loss is: [17.949457] at itertion: 0\n",
            "Validation loss is: [16.294523] at itertion: 0\n",
            "Training loss is: [1.9495711] at itertion: 200\n",
            "Validation loss is: [1.711425] at itertion: 200\n",
            "Training loss is: [1.3530564] at itertion: 400\n",
            "Validation loss is: [1.2506859] at itertion: 400\n",
            "Training loss is: [0.94615835] at itertion: 600\n",
            "Validation loss is: [0.9179513] at itertion: 600\n",
            "Training loss is: [0.6717386] at itertion: 800\n",
            "Validation loss is: [0.67743725] at itertion: 800\n",
            "Model saved in file: /tmp/earthquake_model.ckpt\n",
            "Final training loss: [0.4821202]\n",
            "Final validation loss: [0.49828047]\n"
          ]
        }
      ],
      "source": [
        "#Initialization and session\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    sess.run(init)\n",
        "    print(\"Training loss:\",sess.run([mean_square],feed_dict={X:InputX1train,Y:InputY1train}))\n",
        "    for i in range(training_iterations):\n",
        "        sess.run([train_step],feed_dict={X:InputX1train,Y:InputY1train})\n",
        "        if i%display_iterations ==0:\n",
        "            print(\"Training loss is:\",sess.run([mean_square],feed_dict={X:InputX1train,Y:InputY1train}),\"at itertion:\",i)\n",
        "            print(\"Validation loss is:\",sess.run([mean_square],feed_dict={X:InputX1v,Y:InputY1v}),\"at itertion:\",i)\n",
        "    # Save the variables to disk.\n",
        "    save_path = saver.save(sess, \"/tmp/earthquake_model.ckpt\")\n",
        "    print(\"Model saved in file: %s\" % save_path)\n",
        "\n",
        "    print(\"Final training loss:\",sess.run([mean_square],feed_dict={X:InputX1train,Y:InputY1train}))\n",
        "    print(\"Final validation loss:\",sess.run([mean_square],feed_dict={X:InputX1v,Y:InputY1v}))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oMF-6RNDZo9t",
        "outputId": "d1cd84df-16f4-413c-997a-524106a79316"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Latitude between -77 to 86:45\n",
            "Enter Longitude between -180 to 180:11\n",
            "Enter Depth between 0 to 700:500\n",
            "Enter the date (Month/Day/Year format):10/25/2026\n",
            "INFO:tensorflow:Restoring parameters from /tmp/earthquake_model.ckpt\n",
            "Model restored.\n",
            "output: [array([[8.305732]], dtype=float32)]\n"
          ]
        }
      ],
      "source": [
        "#Testing\n",
        "lat = input(\"Enter Latitude between -77 to 86:\")\n",
        "long = input(\"Enter Longitude between -180 to 180:\")\n",
        "depth = input(\"Enter Depth between 0 to 700:\")\n",
        "date = input(\"Enter the date (Month/Day/Year format):\")\n",
        "InputX2 = np.asarray([[lat,long,depth,mapdateTotime(date)]],dtype=np.float32)\n",
        "InputX2_norm = (InputX2-X1_min)/(X1_max-X1_min)\n",
        "InputX1test = np.resize(InputX2_norm,(1,Xfeatures))\n",
        "with tf.compat.v1.Session() as sess:\n",
        "    # Restore variables from disk for validation.\n",
        "    saver.restore(sess, \"/tmp/earthquake_model.ckpt\")\n",
        "    print(\"Model restored.\")\n",
        "    #print(\"Final validation loss:\",sess.run([mean_square],feed_dict={X:InputX1v,Y:InputY1v}))\n",
        "    print(\"output:\",sess.run([output_layer],feed_dict={X:InputX1test}))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "colab": {
      "name": "EarthquakePredict.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}